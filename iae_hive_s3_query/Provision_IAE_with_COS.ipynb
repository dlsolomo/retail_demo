{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "In this notebook, we use the Bluemix CLI tools to create a new IBM Analytics Engine instance that is configured to use IBM Cloud Object Storage (IBM COS).\n",
    "\n",
    "---\n",
    "\n",
    "## Load utility library and set notebook width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent this notebook from getting too cluttered, we use some python utilities.  We load them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./modules\")\n",
    "import iae_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set this notebook to use the full width of the browser using the utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iae_examples.set_notebook_full_width()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Read Cloud Foundry endpoint properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read some variables saved when we ran the notebook `examples/CLI/CLI_Setup.ipynb` to configure our choosen api, org and space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CF_API, CF_ORG, CF_SPACE) = iae_examples.read_cf_target_endpoint_details('../secrets/cf_target_endpoint.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save IBM Cloud Object Storage endpoint properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a file `../secrets/cos_s3_endpoint.json` with your COS credentials.  The file format should be:\n",
    "\n",
    "```\n",
    "{\n",
    "   \"S3_ACCESS_KEY\":       \"<AccessKey-changeme>\",\n",
    "   \"S3_PRIVATE_ENDPOINT\": \"<Private-EndPoint-changeme>\",\n",
    "   \"S3_PUBLIC_ENDPOINT\":  \"<Public-EndPoint-changeme>\",\n",
    "   \"S3_SECRET_KEY\":       \"<SecretKey-changeme>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the cos file into some variables that we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(S3_ACCESS_KEY, S3_PRIVATE_ENDPOINT, S3_PUBLIC_ENDPOINT, S3_SECRET_KEY) = \\\n",
    "    iae_examples.read_cos_endpoint_details('../secrets/cos_s3_endpoint.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Upload IAE bootstrap file to COS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/snowch/IBM_Analytics_Engine_Examples/master/scripts/COS_S3.sh'\n",
    "filename = 'COS_S3_bootstrap.sh'\n",
    "bucket_name = 'temp-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iae_examples.save_url_to_cos(url, bucket_name, filename, S3_ACCESS_KEY, S3_SECRET_KEY, S3_PUBLIC_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Provision IAE instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can provision IAE, we need to login to Bluemix using the Bluemix CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bx login --apikey @../secrets/apiKeyPersonal.json -a {CF_API} -o {CF_ORG} -s {CF_SPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to configure IAE to use IBM COS.   Let's automate the process with a custom script.\n",
    "\n",
    "**NOTE:** These examples prefer automation to manual approaches for configuration.  One key benefit of automation is that it supports creating environments in a repeatable and testable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "custom_script = { \n",
    "    \"num_compute_nodes\": 1, \n",
    "    \"hardware_config\": \"default\", \n",
    "    \"software_package\": \"ae-1.0-hadoop-spark\",\n",
    "    \"customization\": [{\n",
    "        \"name\": \"action1\",\n",
    "        \"type\": \"bootstrap\",\n",
    "        \"script\": {\n",
    "            \"source_type\": \"CosS3\",\n",
    "            \"source_props\": {\n",
    "                \"auth_endpoint\": S3_PRIVATE_ENDPOINT,\n",
    "                \"access_key_id\": S3_ACCESS_KEY,\n",
    "                \"secret_access_key\": S3_SECRET_KEY\n",
    "             },\n",
    "         \"script_path\": bucket_name + \"/COS_S3_bootstrap.sh\"\n",
    "        },\n",
    "        \"script_params\": [S3_ACCESS_KEY, S3_PRIVATE_ENDPOINT, S3_SECRET_KEY]\n",
    "    }]\n",
    "}\n",
    "\n",
    "# write the script to a file in the local directory where we can access it in the next step using the Bluemix CLI \n",
    "\n",
    "with open('../secrets/custom_script.json', 'w') as f:\n",
    "    f.write(json.dumps(custom_script))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then attempt to create an IBM Analytics Engine Instance using the custom script file that we created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bx cf create-service IBMAnalyticsEngine lite 'myiaeinstance' -c ../secrets/custom_script.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Note the output from above.  If all went ok, the CLI should suggest running `cf service myiaeinstance` to check the provisioning status. Let's do that now.\n",
    "\n",
    "**NOTE:** If there is an error output by the above step, jump to the section below on debugging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bx cf service myiaeinstance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the status is: `create succeeded`, move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create service key\n",
    "\n",
    "Here we create a service key which contains the cluster credentials. \n",
    "We export the service key information to a file. \n",
    "We can then read the service key details into python variables so we can use those variables later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bx cf create-service-key myiaeinstance myiaeinstance_servicekey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bx cf service-keys myiaeinstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! bx cf service-key myiaeinstance myiaeinstance_servicekey > ../secrets/iae_service_key.json\n",
    "\n",
    "# unfortunately, the output of the above command contains some lines of text before the json\n",
    "# lets remove the first four lines of output and save the raw json\n",
    "\n",
    "iae_examples.strip_premable_from_service_key('../secrets/iae_service_key.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IAE_USER        = iae_examples.iae_service_user('../secrets/iae_service_key.json')\n",
    "IAE_PASSWORD    = iae_examples.iae_service_password('../secrets/iae_service_key.json')\n",
    "IAE_AMBARI_URL  = iae_examples.iae_service_endpoint_ambari('../secrets/iae_service_key.json')\n",
    "IAE_LIVY_URL    = iae_examples.iae_service_endpoint_livy('../secrets/iae_service_key.json')\n",
    "IAE_WEBHDFS_URL = iae_examples.iae_service_endpoint_webhdfs('../secrets/iae_service_key.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iae_examples.read_iae_service_keys('../secrets/iae_service_key.json')['cluster']['service_endpoints']['hive_jdbc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verify COS was successfully configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is broken\n",
    "# iae_examples.is_s3_access_key_set(IAE_AMBARI_URL, IAE_USER, IAE_PASSWORD, S3_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create hive table and query it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSH onto the cluster (see ../secrets/iae_service_key.json for endpoints and credentials)\n",
    "\n",
    "```\n",
    "ssh clsadmin@chs-xxxxx-mn003.bi.services.us-south.bluemix.net\n",
    "beeline -u 'jdbc:hive2://chs-xxxxx-mn001.bi.services.us-south.bluemix.net:8443/;ssl=true;transportMode=http;httpPath=gateway/default/hive' -n clsadmin -p **********\n",
    "```\n",
    "\n",
    "Next from the beeline session, create the hive table:\n",
    "    \n",
    "```\n",
    "CREATE EXTERNAL TABLE avro_hive_table\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n",
    "LOCATION 's3a://temp-bucket/transactions/'\n",
    "TBLPROPERTIES (\n",
    "    'avro.schema.literal'='{\n",
    "                            \"namespace\": \"transaction.avro\",\n",
    "                                     \"type\": \"record\",\n",
    "                                     \"name\": \"Transaction\",\n",
    "                                     \"fields\": [\n",
    "                                         {\"name\": \"InvoiceNo\",     \"type\": \"int\"    },\n",
    "                                         {\"name\": \"StockCode\",     \"type\": \"string\" },\n",
    "                                         {\"name\": \"Description\",   \"type\": \"string\" },\n",
    "                                         {\"name\": \"Quantity\",      \"type\": \"int\"    },\n",
    "                                         {\"name\": \"InvoiceDate\",   \"type\": \"long\"   },\n",
    "                                         {\"name\": \"UnitPrice\",     \"type\": \"float\"  },\n",
    "                                         {\"name\": \"CustomerID\",    \"type\": \"int\"    },\n",
    "                                         {\"name\": \"Country\",       \"type\": \"string\" },\n",
    "                                         {\"name\": \"LineNo\",        \"type\": \"int\"    },\n",
    "                                         {\"name\": \"InvoiceTime\",   \"type\": \"string\" },\n",
    "                                         {\"name\": \"StoreID\",       \"type\": \"int\"    },\n",
    "                                         {\"name\": \"TransactionID\", \"type\": \"string\" }\n",
    "                                     ]\n",
    "                            }'\n",
    "    )\n",
    ";\n",
    "```\n",
    "\n",
    "And query:\n",
    "\n",
    "```\n",
    "select count(*) from avro_hive_table;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Upload spark script to COS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a pyspark script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = \"\"\"\n",
    "from __future__ import print_function\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()\n",
    "\n",
    "    output = \"Hello World at %s\" % (str(datetime.now()))\n",
    "    print(output)\n",
    "\n",
    "    output_rdd = spark.sparkContext.parallelize([output])\n",
    "    output_rdd.coalesce(1, True).saveAsTextFile('s3a://{0}/provision_iae_with_cos_spark_job_output.txt')\n",
    "\n",
    "    spark.stop()\n",
    "\"\"\".format(bucket_name)\n",
    "\n",
    "bucket_name = 'temp-bucket'\n",
    "filename = 'PiEx.py'\n",
    "\n",
    "(S3_ACCESS_KEY, S3_PRIVATE_ENDPOINT, S3_PUBLIC_ENDPOINT, S3_SECRET_KEY) = \\\n",
    "    iae_examples.read_cos_endpoint_details('../secrets/cos_s3_endpoint.json')\n",
    "\n",
    "iae_examples.save_string_to_cos(\n",
    "    file_contents.encode('ascii'), bucket_name, filename, S3_ACCESS_KEY, S3_SECRET_KEY, S3_PUBLIC_ENDPOINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submit spark job with Livy API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAE_USER        = iae_examples.iae_service_user('../secrets/iae_service_key.json')\n",
    "IAE_PASSWORD    = iae_examples.iae_service_password('../secrets/iae_service_key.json')\n",
    "IAE_LIVY_URL    = iae_examples.iae_service_endpoint_livy('../secrets/iae_service_key.json')\n",
    "IAE_WEBHDFS_URL = iae_examples.iae_service_endpoint_webhdfs('../secrets/iae_service_key.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the `bucket_name` variable is still set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "headers = { \n",
    "    'Content-Type': 'application/json',\n",
    "    'X-Requested-By': 'livy'\n",
    "}\n",
    "data = { \"file\":\"s3a://{0}/PiEx.py\".format(bucket_name) }\n",
    "\n",
    "res = requests.post(IAE_LIVY_URL, auth=(IAE_USER, IAE_PASSWORD), headers=headers, data=json.dumps(data))\n",
    "print(res.text)\n",
    "\n",
    "id = res.json()['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we saved the job `id` in the previous step. Let's use that to.  Let's use that to get job state.\n",
    "\n",
    "**NOTE:** keep running the cell below until status is successful or it has failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { \n",
    "    'Content-Type': 'application/json',\n",
    "    'X-Requested-By': 'livy'\n",
    "}\n",
    "url = '{0}/{1}'.format(IAE_LIVY_URL, id)\n",
    "response = requests.get(url, auth=(IAE_USER, IAE_PASSWORD), headers=headers)\n",
    "print(response.json()['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the spark job log using the job `id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = { \n",
    "    'Content-Type': 'application/json',\n",
    "    'X-Requested-By': 'livy'\n",
    "}\n",
    "url = '{0}/{1}/log'.format(IAE_LIVY_URL, id)\n",
    "response = requests.get(url, auth=(IAE_USER, IAE_PASSWORD), headers=headers)\n",
    "print('\\n'.join(response.json()['log']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Debugging errors\n",
    "\n",
    "This is the processed I followed to debug an issue with my submitted job.\n",
    "\n",
    "If there is an error we can debug by looking for the yarn application id in the log output, e.g.\n",
    "\n",
    "```\n",
    "17/09/23 06:21:51 INFO Client: Application report for application_1506108548102_0002 (state: ACCEPTED)\n",
    "```\n",
    "\n",
    "Ssh onto the cluster, I ran the following command (change for your application_xxxx value):\n",
    "\n",
    "```\n",
    "$ yarn logs -applicationId application_1506108548102_0002 | less\n",
    "```\n",
    "\n",
    "Burried in the yarn output, I noticed the following:\n",
    "\n",
    "```\n",
    "py4j.protocol.Py4JJavaError: An error occurred while calling o76.saveAsTextFile.\n",
    ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory s3a://temp-bucket/provision_iae_with_cos_spark_job_output.txt already exists\n",
    "```\n",
    "---\n",
    "\n",
    "## Print out the contents of the COS file created by Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the file contents.  In the spark job, we coalesced the RDD.  This causes spark to just save  one output file, which will be part-00000:\n",
    "\n",
    "```\n",
    "output_rdd.coalesce(1, True).saveAsTextFile('s3a://{0}/provision_iae_with_cos_spark_job_output.txt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iae_examples.get_file_content_from_cos(\n",
    "                bucket_name, \n",
    "                'provision_iae_with_cos_spark_job_output.txt/part-00000', \n",
    "                S3_ACCESS_KEY, S3_SECRET_KEY, S3_PUBLIC_ENDPOINT\n",
    "                )\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep things tidy and remove our job output.  Note that if you don't remove it, next time you run the spark job it will fail because the output file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iae_examples.recursively_delete_file_in_cos(\n",
    "                bucket_name, \n",
    "                'provision_iae_with_cos_spark_job_output.txt', \n",
    "                S3_ACCESS_KEY, S3_SECRET_KEY, S3_PUBLIC_ENDPOINT\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Debugging \n",
    "\n",
    "TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! bx cf space dev --guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! bx cf services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! bx cf service-keys myiaeinstance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bx cf delete-service-key myiaeinstance myiaeinstance_servicekey -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bx cf delete-service myiaeinstance -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
